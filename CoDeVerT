Graph-Based Conflict Detection for Versioned Text (CoDeVerT)

Abstract

>Introduction

In the digital age, many important repositories of collective knowledge have become unreliable. The emergence of crowdsourcing as a form of knowledge-production and the internet as a global commons have greatly increased the prevalence of collaboratively written documents with hazy or entirely anonymous attribution. However, the codevelopment of version-control tools with canonical logs has provided us with the means to thoroughly investigate the processes by which these new documents are created and evolve over time, their "provenance." And in analyzing this often messy and chaotic provenance, we can better establish the integrity and reliability of heretofore opaque sources of information. The research presented in this paper parses Wikipedia edit histories and develops a quantitative way to describe edit wars — which for exposition purposes we’re going to call controversy, while we fully acknowledge that use of the term itself might be controversial. When reviewing some versioned body of text - an article edit history, an evolving standards document, or an active code base - one might want to know which parts were most contentious, where authors vehemently and vocally disagreed. This information could help readers evaluate the relative trustworthiness/reliability of a given section, sentence, or individual fact and editors identify where their services are most needed.

In this paper we propose a technique for calculating the relative controversy of text segments through a document's whole edit history. We posit that the controversy of a text segment is the total semantic distance it travels across consecutive edits within some context-appropriate span of time. The intuition here is that some word/phrase/sentence/paragraph is controversial if it has been repeatedly modified in ways that significantly alter its overall meaning. To make the problem tractable, we convert a document edit history into a patch model. This structure is a directed acyclic graph (DAG) where vertices are contiguous changes to the text - which we call patches - and edges represent dependencies of new patches on old ones. A particular revision can contribute multiple insert or delete patches to the model. Subsequently, we can estimate the controversy of any node as its height in some time-bounded subgraph. We use these unit-less height scores to assign any text segment to a controversy quantile both within and across documents.

>The Patch Model

At a high level, a patch model takes the full history of some versioned document as input, constructs a graph that represents changes to the article over time, and assigns scores to individual words or text segments based on properties of their corresponding vertices in the graph. Any versioned document can be processed into a patch model, but there isn't a fully general solution for every kind of text. We will describe our implementation for Wikipedia articles, emphasizing where alternate decisions could have been made to suit other corpora (e.g. Linux source code, for which a working implementation also exists).
	
	>Isolating Patches

At the core of the patch model is the notion of a basic unit of change. First, let a revision refer to a specific Wikipedia article version. A new version is produced every time article edits are submitted to Wikipedia. A patch is a contiguous set of either adds or deletes between two revisions, along with a record of their starting position; an addition patch contains only adds and a deletion patch only deletions. The difference between two revisions is composed of a patchset, a list of patches necessary to construct one revision from the other. [Figure Here]

		>Diff Algorithm

The choice of diff algorithm greatly affects the form of resulting patches:
	Word vs Line
	Myers vs. Patience

		>Invalid Revisions

Some revisions do not contribute to the 


	>Building the Graph
	
		>Piece Tables

	>Assigning Edge Weights

	>Scoring Patches



We convert every document version into a vector of 'words' corresponding to logical subdivisions of the document. For plain-text, these are naturally just actual words, but for code, we use an AST to split the document into syntactically meaningful chunks. 

We diff each document version vector with its immediate predecessor, giving us a series of 'patch sets.' A patch set is a list of PATCHES, either INSERTS and DELETES. A PATCH has a unique identifier (PATCH.id), length in words (PATCH.len), and position in the earlier document version (PATCH.pos).

Now we iterate once through the document versions, building two objects: a DEH graph and the PATCH-to-range list on which it depends. The P2P list contains position, size, PATCH.id triples. These objects represent contiguous spans of words belonging to the same patch. DELETES have size 0, but are still represented in the P2P list. The DEH graph is a DAG. Each PATCH is a unique vertex. Edges reflect dependencies between patches and have a Weight and Prob attributes.
 
At first, our P2P list has a single entry representing the entire original document version (0, V_0.len, 0) and our graph has no edges. The update procedure is somewhat complicated.

for each PATCH in the new version's patch set, in descending order by PATCH.pos, ensuring DELETES are processed first [so that replaces register correctly]:

	if the patch is an INSERT: 
	- Add the patch to the P2P list with size = PATCH.size
	- If the patch interrupts an existing P2P entry, ie pos < PATCH.pos < pos + size, split that entry around the new PATCH entry. So the first entry is (pos_0, PATCH.pos - pos_0, id) and the second (PATCH.pos + PATCH.size, size_0 - (PATCH.pos - pos_0), id)
	-Remove any DELETES whose pos = INSERT.pos
	- for every pos in P2P > PATCH.pos, pos += PATCH.size

		What does this mean in terms of the graph:
		- If the patch interrupted an existing P2P entry, just draw a dependency from the new patch to the old patch.
		- If the patch fell between two entries, draw a dependency to each. [explain intuition here]
		- If for any DELETE, PATCH.pos == DELETE.pos, draw a dependency from PATCH to that DELETE.



	if the patch is a DELETE:
	- Add the patch with size = 0
	[check if a delete_n subsumes deletes where pos = delete_n.pos, can't remember, i think so :)]
	- For every pos in P2P > PATCH.pos if PATCH is an
